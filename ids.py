# -*- coding: utf-8 -*-
"""ids.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19_WrjRmxDPHh8TzMcPogDFSSJAg_Dl2T

**Visualizing and predicting the wine type based on wine characteristics**
"""

import pandas as pd

red_wine_data_raw = pd.read_csv("winequality-red.csv",delimiter = ';')
white_wine_data_raw = pd.read_csv("winequality-white.csv",delimiter = ';')

red_wine_data_raw[:10]

white_wine_data_raw[:10]

print(white_wine_data_raw.count())

print(red_wine_data_raw.count())

white_wine_data_raw.describe()

red_wine_data_raw.describe()

white_wine_data_raw['Type'] = 1
red_wine_data_raw['Type'] = 0

white_wine_data_raw[:5]

red_wine_data_raw[:5]

white_wine_data_raw.mean()

red_wine_data_raw.mean()

"""# Checking for Null Values

  Here we check if there are any null values in the dataset attributes.
"""

white_wine_data_raw.isnull().sum()

red_wine_data_raw.isnull().sum()

"""As can be seen from above 2 results, there are no null values in both the dataset.

# Analysing the variation of various features in both type of wines
"""

import seaborn as sns
from matplotlib import pyplot as plt

import numpy as np 
barWidth = 0.25
fig = plt.subplots(figsize =(20, 9))
red_m = red_wine_data_raw.mean()
red_m = red_m.drop(labels=['free sulfur dioxide','total sulfur dioxide','Type','quality'])
white_m = white_wine_data_raw.mean()
white_m = white_m.drop(labels=['free sulfur dioxide','total sulfur dioxide','Type','quality'])
br1 = np.arange(len(red_m)) 
br2 = [x + barWidth for x in br1] 
p1 = plt.bar(br1, red_m, color ='r', width = barWidth, 
        edgecolor ='grey', label ='RED WINE') 
p2 = plt.bar(br2, white_m, color ='g', width = barWidth, 
        edgecolor ='grey', label ='WHITE WINE') 
plt.xticks([r + barWidth for r in range(len(red_m))], 
           ['fixed acidity', 'volatile acidity ', 'citric acid', 'residual sugar', 'chlorides','density','pH','sulphates','alcohol']) 
plt.xticks(fontsize=16)
plt.legend((p1, p2), ('Red Wine', 'White Wine'))

barWidth = 0.1
fig = plt.subplots(figsize =(20, 10))
red_m1 = red_wine_data_raw.mean()
red_m = red_m1[5:7]
white_m1 = white_wine_data_raw.mean()
white_m = white_m1[5:7]
br1 = np.arange(len(red_m)) 
br2 = [x + barWidth for x in br1] 
p1 = plt.bar(br1, red_m, color ='r', width = barWidth, 
        edgecolor ='grey', label ='RED WINE') 
p2 = plt.bar(br2, white_m, color ='g', width = barWidth, 
        edgecolor ='grey', label ='WHITE WINE') 
plt.xticks([r + barWidth for r in range(len(red_m))], 
           ['free sulfur dioxide','total sulfur dioxide'])
plt.xticks(fontsize=16) 
plt.legend((p1, p2), ('Red Wine', 'White Wine'))

sns.set_theme(style="whitegrid")
ax=plt.subplots(figsize=(15,7))
ax = sns.boxplot(x=red_wine_data_raw['pH'], orient = 'vertical')

ax=plt.subplots(figsize=(15,7))
ax = sns.boxplot(x=white_wine_data_raw['pH'], orient = 'vertical')

ax=plt.subplots(figsize=(15,7))
ax = sns.boxplot(x=red_wine_data_raw['alcohol'], orient = 'vertical')

ax=plt.subplots(figsize=(15,7))
ax = sns.boxplot(x=white_wine_data_raw['alcohol'], orient = 'vertical')

ax=plt.subplots(figsize=(15,7))
ax = sns.boxplot(x=red_wine_data_raw['density'], orient = 'vertical')

ax=plt.subplots(figsize=(15,7))
ax = sns.boxplot(x=white_wine_data_raw['density'], orient = 'vertical')

"""<b>Both Alcohol and density can be dropped from the dataframes since they have nearly same statistical distribution of values. This helps in reducing the complexity of the classification model.<b> """

#We will first Merge the two DataFrames before dropping the columns

merged_df=pd.concat([white_wine_data_raw,red_wine_data_raw])
merged_df[:10]

(merged_df.where(merged_df['Type']==1)).count()

#Dropping alchohol,density and quality columns from the dataframe
merged_df=merged_df.drop(['alcohol','density','quality'],axis=1)
merged_df

"""**We will use Standard Scaler to Standardize the data** """

from sklearn.preprocessing import StandardScaler
x=merged_df.drop(['Type'],axis=1)
y=merged_df['Type']
scaled_x=StandardScaler().fit_transform(x)
scaled_x

"""**Splitting dataset into training and testing set**"""

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(scaled_x,y,random_state=0)

"""**Creating a function for GridSearch**

"""

def GridSearch(cls,param,X_train=X_train,y_train=y_train):
    from sklearn.model_selection import GridSearchCV
    grid=GridSearchCV(cls,param_grid=param,cv=5)
    grid.fit(X_train,y_train)
    return grid

"""**KNearestNeighbour Classifier**

We use KNearest Neighbour classifier to classify our data. KNeighboursClassifier is imported from the sklearn.neghbours.
"""

from sklearn.neighbors import KNeighborsClassifier
knn=KNeighborsClassifier()
param={'n_neighbors':[2,4,5,6,8,10,15,20,40,60,100]}
knn_grid=GridSearch(knn,param)
print(knn_grid.best_estimator_)
print(knn_grid.best_params_)
print(knn_grid.best_index_)
knn_grid.score(X_test,y_test)

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
y_pred_knn = knn_grid.predict(X_test)
c_m = confusion_matrix(y_test, y_pred_knn)
print('Confusion matrix plot')
sns.heatmap(confusion_matrix(y_test,y_pred_knn),annot=True, fmt="d" )
plt.xlabel('Predicted ')
plt.ylabel('True')
plt.show()
print("Classfication report")
print(classification_report(y_test, y_pred_knn))
print("Accuracy score")
print(accuracy_score(y_test,y_pred_knn))